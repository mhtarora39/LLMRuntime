# LLMRuntime


## Why we need position embedding ? 
</br>
To add the sense of position in the network.
</br>

## Can it be learned ?
Yes we can but model will not able to extrapolate for longer sequence length.
</br>

## Types of positional embedding?

* Sinusoidal Positional Encoding! [ -> Not easy to extrapolate.] Demo : https://github.com/mhtarora39/LLMRuntime/blob/main/experiments/PosEmbeddings.py

* Rotatory Positional Embedding! [ -> Better ]

## Difference Between RMSNorm and LayerNorm.

* https://github.com/mhtarora39/LLMRuntime/blob/main/model/model.py#L19




