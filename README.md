# LLMRuntime

## Setup Embedding Layer
## Reading Position Embedding.

## Why we need position embedding ? 
</br>
To add the sense of position in the network.
</br>

## Can it be learned!
## Yes we can but model will not able to extrapolate for longer sequence length.

## Types of positional embedding!
## Sinusoidal Positional Encoding! [ -> Not easy to extrapolate.] 
## Rotatory Positional Embedding! [ -> Better ]




